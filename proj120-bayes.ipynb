{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Richa176/PythonProj/blob/main/proj120-bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5dZWtSG8BvW"
      },
      "source": [
        "Studnet side code "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo8VwKkTFsx8"
      },
      "source": [
        "#Uploading the csv\n",
        "from google.colab import files\n",
        "data_to_load = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2afzLZAIvTk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "3a0c00a4-b506-40d7-bb8c-930fda3e6fbe"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('income.csv')\n",
        "\n",
        "print(df.head())\n",
        "print(df.describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   age          workclass  ...  native-country  income\n",
            "0   39          State-gov  ...   United-States   <=50K\n",
            "1   50   Self-emp-not-inc  ...   United-States   <=50K\n",
            "2   38            Private  ...   United-States   <=50K\n",
            "3   53            Private  ...   United-States   <=50K\n",
            "4   28            Private  ...            Cuba   <=50K\n",
            "\n",
            "[5 rows x 14 columns]\n",
            "                age  education-num  capital-gain  capital-loss  hours-per-week\n",
            "count  45222.000000   45222.000000  45222.000000  45222.000000    45222.000000\n",
            "mean      38.547941      10.118460   1101.430344     88.595418       40.938017\n",
            "std       13.217870       2.552881   7506.430084    404.956092       12.007508\n",
            "min       17.000000       1.000000      0.000000      0.000000        1.000000\n",
            "25%       28.000000       9.000000      0.000000      0.000000       40.000000\n",
            "50%       37.000000      10.000000      0.000000      0.000000       40.000000\n",
            "75%       47.000000      13.000000      0.000000      0.000000       45.000000\n",
            "max       90.000000      16.000000  99999.000000   4356.000000       99.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_zI9gQlKMZU"
      },
      "source": [
        "From the given data, we will consider the following fields to determine the salary of a person -\n",
        "\n",
        "\n",
        "\n",
        "1.   Age\n",
        "2.   Hours Per Week\n",
        "3.   Education Number\n",
        "4.   Capital Gain\n",
        "5.   Capital Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHP4vXJ8JsSP"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[[\"age\", \"hours-per-week\", \"education-num\", \"capital-gain\", \"capital-loss\"]]\n",
        "y = df[\"income\"]\n",
        "\n",
        "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ommNAshKLqWz"
      },
      "source": [
        "## Training the model with Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQqZZStNLhGi"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "x_train_1 = sc.fit_transform(x_train_1) \n",
        "x_test_1 = sc.fit_transform(x_test_1) \n",
        "\n",
        "model_1 = GaussianNB()\n",
        "model_1.fit(x_train_1, y_train_1)\n",
        "\n",
        "y_pred_1 = model_1.predict(x_test_1)\n",
        "\n",
        "accuracy = accuracy_score(y_test_1, y_pred_1)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPURq8TwMhSp"
      },
      "source": [
        "This time, with the new dataset, we can see that Naive Bayes gave us an accuracy of almost **79%**. Let's see how much accuracy do we get with Logistic Regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAs37hE_Ludo"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[[\"age\", \"hours-per-week\", \"education-num\", \"capital-gain\", \"capital-loss\"]]\n",
        "y = df[\"income\"]\n",
        "\n",
        "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGXuObEUNXpH"
      },
      "source": [
        "## Training model with Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQKN9r-rNO8G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04d9363f-136c-48d2-bdfe-ae3968dfe36c"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "x_train_2 = sc.fit_transform(x_train_2) \n",
        "x_test_2 = sc.fit_transform(x_test_2) \n",
        "\n",
        "model_2 = LogisticRegression(random_state = 0) \n",
        "model_2.fit(x_train_2, y_train_2)\n",
        "\n",
        "y_pred_2 = model_2.predict(x_test_2)\n",
        "\n",
        "accuracy = accuracy_score(y_test_2, y_pred_2)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8116929064213692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBUftfATNcc8"
      },
      "source": [
        "With Logistic Regression, this time, we got an accuracy of **81.1%**. Let's study this more closely.\n",
        "\n",
        "\\\n",
        "# Difference b/w Naive Bayes and Logistic Regression\n",
        "\n",
        "\\\n",
        "In the first dataset, as we pointed out earlier, both the *glucose* and the *bloodpressure* had little correlation, and both of them were contributing individually to whether a person has diabetes or not.\n",
        "\n",
        "\\\n",
        "**Conclusion**\n",
        "In these kinds of dataset, where all the features contribute individually to the outcome, Naive Bayes outperforms logistic regression and is highly efficient.\n",
        "\n",
        "\\\n",
        "In the second dataset, Logistic Regression outperformed Naive Bayes. The reason is that in this dataset, not all features contribute individually to the outcome. For example, there have been people of all age groups earning both less than and more than 50K. There have also been people with all education numbers that have an income of both less and more than 50K. Here, the combination of all the features is a better predictor of whether a person is earning more than or less than 50K, instead of all features having their individual contribution."
      ]
    }
  ]
}